import requests
from bs4 import BeautifulSoup

import mysql.connector

# website_analyzed is the name for the website 
website_analyzed = ""
mydb = mysql.connector.connect(
    host="localhost",
    user="root",
    passwd="PZZwsmapE4-Uk:6"
)
mycursor = mydb.cursor()
sqlFormula = ""

#URL to be scraped
url_to_scrape = 'https://www.politico.com/magazine/story/1'
# Load html's plain data into a variable
# NOTE: .text returned a string with bad encoding (maybe cp1252) -- .content.decode('utf-8') 
#       returns HTML as bytes (.content), then converts them to string with 
#       proper encoding (.decode('utf-8'))
#       SOURCE: https://stackoverflow.com/a/65027863
plain_html_text = requests.get(url_to_scrape).content.decode('utf-8')
#parse the data
soup = BeautifulSoup(plain_html_text, 'html.parser')
#get list of all articles
articles = soup.find_all('article')

def create_tuple():
    url_list = create_url_list()
    scraped_info = []
    for url in url_list:
        plain_text_html = requests.get(url).content.decode('utf-8')
        soup = BeautifulSoup(plain_text_html, 'html.parser')
        articles = soup.find_all('article')

        for article in articles:
            article_tuple = tuple((get_category(article), get_title(article), get_authors(article),
                                    get_link(article), get_subheader(article), get_date_time(article)))
            scraped_info.append(article_tuple)

    print(len(scraped_info))
    return scraped_info

## Returns list of urls comprising of all politico archives
## TODO: try/except for page 348 links
def create_url_list():
    
    url_list = []
    url = 'https://www.politico.com/magazine/story/{}'
    pages = 686
    for i in range(1, pages):
        try:
            url_list.append(url.format(i))
        except:
            print('Ran into problem with page: ' + i + '\n')
    return url_list    


## Prints out catagory of article
def get_category(article):
    if (len(article.find_all("p", class_ = "category")) == 1):
        return article.find_all("p", class_ = "category")[0].get_text()

## Prints out subheader for all article in articles 
def get_subheader(article):
    if (len(article.find_all("p", class_ = "subhead")) == 1):
        return (article.find_all("p", class_ = "subhead")[0].string)

## Prints out link for article
## TODO: try/except for page 348 links
def get_link(article):
    return article.h3.get('href')

def get_title(article):
    return article.h3.string

## Prints out authors of article
## TODO: store as "author1, author2"
def get_authors(article):
    span_list = article.footer.find_all("span", class_ = "vcard")
    authors = []
    for name in span_list:
        authors.append(name.string)
    return authors

def get_date_time(article):
    ## returns as MM/DD/YY  00:00 XM EST
    return article.time.string
    ## returns as YYYY-MM-DDT00:00-0000 (not sure exactly what format is -- run to check)
    # return article.time.get('datetime')

create_tuple()

# Initiates the database and adds the data into it. If there is already a database, it adds the elements to the existing database
# scrapedFromWebsite is a list containing tuples generated by the scraper Ex: list((string, string))
# website is a string containing an identifier for the website Ex: wsj.com
def initiate_database(scrapedFromWebsite, website):
    #Check that the proper database exists
    mycursor.execute("SHOW DATABASES LIKE 'headlinedb'")
    #if database does not exist, create one
    if (len(mycursor.fetchall()) == 0):
        mycursor.execute("CREATE DATABASE headlinedb")

    mycursor.execute("USE headlinedb") #@MARKER

    website_analyzed = website

    mycursor.execute("SHOW TABLES LIKE '" + website_analyzed + "'")
    if (len(mycursor.fetchall()) == 0):
        ## TODO: Figure out table columns
        ##       - title
        ##       - url
        ##       - type (catagory)
        ##       - date (day/month/year might be better in seperate columns to make for easy sorting of data)
        mycursor.execute("CREATE TABLE " + website_analyzed + " (date VARCHAR(255), headline VARCHAR(255))")

    sqlFormula = "INSERT INTO " + website_analyzed + "(date, headline) VALUES (%s, %s)"

    #uses the sqlFormula to put the data in the database
    mycursor.executemany(sqlFormula, scrapedFromWebsite)
    mydb.commit()

# Prints all the data to the standard output location (terminal)
def printAllData():
    mycursor.execute("SELECT * FROM wsjcom")

    for row in mycursor.fetchall():
        print(row)

# add the element to the database
def addToDatabase(listOfTuple):
    mycursor.executemany(sqlFormula, listOfTuple)
    mydb.commit()

# delete the entries in the database with the given string date
# yearMonthDay is a string containing the date to delete Ex: "2000/01/01"
def deleteAllAtDate(yearMonthDay):
    mycursor.execute("DELETE FROM " + website_analyzed + " WHERE date = '" + yearMonthDay + "'")
    mydb.commit()

# delete the entries in the database with the given snippet
# snippet is a string. Any headline that contains this string will be deleted Ex: "the" <- will delete any entry containing 'the'
def deleteHeadlineContaining(snippet):
    mycursor.execute("DELETE FROM " + website_analyzed + " WHERE headline = '%" + snippet + "%'")
    mydb.commit()

# delete the database and all of its information
def clearDatabase():
    mycursor.execute("DROP DATABASE headlinedb")



# Example list generated by web scraper
exampleScrape = [("2019/08/17", "Fireman dies in wilfire"),
                      ("2015/12/12", "Bird eats a worm that turned out to be a tapeworm"),
                      ("2013/01/30", "Man loses fertility after a 5 day erection"),
                      ("2010/05/26", "Time news begins to make magazines"),
                      ("2005/09/01", "TOTAL DRAMA ACTION! New TV show appears")]

# ADD COMMANDS HERE (be careful if you dont use initiate database you need to set the cursor to the database using @MARKER)
# initiate_database(exampleScrape, "wsjcom")
# mycursor.execute("USE headlinedb")
# clearDatabase()

# printAllData()

